---
title: "1992 U.S. Presidential election"
author: "Ali Tarek Maher Ibrahim Ali Seada and Paul Lovis Maximilian Tr√ºstedt"
date: "24 6 2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Read the data into R environment
```{r}
library(pacman)
p_load(ggplot2,   # reportable graphs 
       cowplot,   # arranges ggplot graphs nicely
       stargazer, # nice tables
       glmnet, # for regularization (lasso, ridge, elastic net)
       caret,       #  splitting the data and more
       rpart,       #  building decision trees 
       rpart.plot,
       pROC)      # ROC AUC
rm(list=ls())
vote<-read.csv("vote92.csv", sep = ",", header = T,stringsAsFactors = T)
str(vote)
summary(vote)
# ??remove cowplot, stargazer & pROC
```

## Preprocess the data, preparing it for the modeling
```{r}
vote$vote_num <- as.numeric(vote$vote)
vote$dem<-as.factor(vote$dem)
vote$rep<-as.factor(vote$rep)
vote$female<-as.factor(vote$female)
vote$persfinance<-as.factor(vote$persfinance)
vote$natlecon<-as.factor(vote$natlecon)
vote$polID<-as.factor((as.numeric(vote$dem)-1)+(as.numeric(vote$rep)*2-1))
str(vote)
```
We decided to change some of the numeric variables to factors, because it makes more sense to have them as categorical than as numeric variables.
Also this way, we can see, that there are no problems with the categorical variables regarding wrong values, because all provided levels are described by the given data set definition.
Additionally we create a categorical variable called polID to summarize which political party the respondent is identifying himself with.

- treat missing values
```{r}
colSums(is.na(vote))
```
There are no missing values in this data set. No NAs, as well as data, that could otherwise be identified as missing.

- handle sparse classes of categorical predictors
```{r}
table(vote$vote) # !!make these tables pretty (bar plot coloured)
table(vote$dem)
table(vote$rep)
table(vote$female)
table(vote$persfinance)
table(vote$natlecon)
vote$natlecon[vote$natlecon==1]<-0
vote$natlecon[vote$natlecon==-1]<-1
vote$natlecon=droplevels(vote$natlecon)
table(vote$natlecon)
```
We leave everything as is except for natlecon which has a sparse class regarding the level 1.
As solution we combine 0 and 1 as the level 0, meaning national economic conditions have gotten better or stayed the same over the last 12 months.
Level -1 gets changed to 1 as well which now means that conditions have gotten better.
The change from -1 to 1 is executed just because it is more common to have levels 0 and 1 instead of 0 and -1.

- take care of outliers, treat the skewed distributions and create new features 
```{r}
zScores<-function(var) {
    mu<-mean(var)
    sd<-sd(var)
    return((var-mu)/sd)
}

# treating clintondis
tp1<-ggplot(vote,aes(clintondis))+geom_boxplot()+coord_flip()
vote$clintondis_fo<-vote$clintondis
vote$clintondis_fo[zScores(vote$clintondis_fo)>1]<-
    round(mean(vote$clintondis_fo))+sd(vote$clintondis_fo)
tp2<-ggplot(vote,aes(clintondis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)

# treating bushdis
tp1<-ggplot(vote,aes(bushdis))+geom_boxplot()+coord_flip()
vote$bushdis_fo<-vote$bushdis
vote$bushdis_fo[zScores(vote$bushdis_fo)>2]<-
    round(mean(vote$bushdis_fo))+2*sd(vote$bushdis_fo)
tp2<-ggplot(vote,aes(bushdis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)

# treating perotdis
tp1<-ggplot(vote,aes(perotdis))+geom_boxplot()+coord_flip()
vote$perotdis_fo<-vote$perotdis
vote$perotdis_fo[zScores(vote$perotdis_fo)>1]<-
    round(mean(vote$perotdis_fo))+sd(vote$perotdis_fo)
tp2<-ggplot(vote,aes(perotdis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)

# !!make this shit beautiful
# !!reduce the outlier fixes with function
```
There are a few outliers in the variables clintondis, bushdis and perotdis. We fix those outliers and save the fixed data in the variables called [original_var_name]_fo. The ending "fo" is derived from "fixed outliers".

- explore the relationships between predictors and the target
```{r}
ggplot(vote,aes(vote,fill=polID))+geom_bar() # !!fix colours and descriptions
# !!add percentiles to those splitted barplots somehow
```
___INSERT___

```{r}
ggplot(vote,aes(vote,fill=female))+geom_bar()
```
___INSERT___

```{r}
p1<-ggplot(vote,aes(vote,fill=persfinance))+geom_bar()
p2<-ggplot(vote,aes(vote,fill=natlecon))+geom_bar()
plot_grid(p1,p2,ncol=2)
# !!fix repeating barplot by creating a more diverse visual representation
```
___INSERT___

```{r}
p1<-ggplot(vote,aes(clintondis_fo,fill=vote))+geom_histogram(bins=3)
p2<-ggplot(vote,aes(bushdis_fo,fill=vote))+geom_histogram(bins=3)
p3<-ggplot(vote,aes(perotdis_fo,fill=vote))+geom_histogram(bins=3)
p4<-ggplot(vote,aes(clintondis,fill=vote))+geom_histogram(bins=3)
p5<-ggplot(vote,aes(bushdis,fill=vote))+geom_histogram(bins=3)
p6<-ggplot(vote,aes(perotdis,fill=vote))+geom_histogram(bins=3)
plot_grid(p1,p2,p3,p4,p5,p6,ncol=3)
# !!fix legend print and axis descriptions, also colours
```
___INSERT___

```{r}
str(vote)
```


## Building the models
```{r}
# splitting the data into train and test
set.seed(777)
train.Index <-  sample(1:nrow(vote), round(0.7*nrow(vote)), replace = F)
# creating the train and test sets using train.Index 
vote.train <- vote[train.Index,]
vote.test  <- vote[-train.Index,]
  
# creating x and y for model training
# y - a target vector
y.train <- vote.train$vote_num
y.test  <- vote.test$vote_num

# X - a matrix with features/predictors 
features <- c('dem','rep','female','persfinance','natlecon', 
              'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')


#model.matrix( ~ ., data = scoring.train[, features])
X.train <- model.matrix( ~ . -1, data = vote.train[, features])
X.test  <- model.matrix( ~ . -1, data = vote.test[, features])

```


1.  L1-nrom / Lasso

```{r}
log_l1 <- glmnet(X.train, y.train, alpha = 1)

plot(log_l1, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X.train), cex = .4)

plot(y = log_l1$dev.ratio, 
     x = log_l1$lambda,
     xlab = "lambda",
     ylab = "R-squared")


# selecting the optimal lambda
set.seed(77)
log_l1_cv <- cv.glmnet(X.train, y.train, alpha = 1, type.measure = "class", 
                       lambda = 10^seq(-5, 1, length.out = 100) , nfolds = 10)



y.predlog_l1 <-  predict(log_l1, newx = X.test, 
                         type = "response", s = log_l1_cv$lambda.min)
```

```{r}
# Setting alpha = 0 implements ridge regression
log_r1 <- glmnet(X.train, y.train, alpha = 0)


plot(log_r1, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X.test), cex = .3)


plot(y = log_r1$dev.ratio, 
     x = log_r1$lambda,
     xlab = "lambda",
     ylab = "R-squared")

# selecting the optimal lambda
set.seed(77)
log_r1_cv <- cv.glmnet(X.train, y.train, alpha = 0, type.measure = "class", 
                       lambda = 10^seq(-5, 1, length.out = 100), 
                        nfolds = 10)

y.predlog_r1 <-  predict(log_r1,    newx = X.test, 
                         type = "response", s = log_r1_cv$lambda.min)

```

```{r}
#only catagorical 

log1 <- glm(vote_num ~  dem + rep + female + persfinance + 
              natlecon + clintondis_fo + persfinance + perotdis_fo +bushdis_fo,
           data = vote)
#both
log2 <- glm(vote_num ~ dem + rep + female +persfinance +
          natlecon ,
           data = vote)

#only continous
log3 <- glm(vote_num ~ clintondis_fo + bushdis_fo +
              perotdis_fo,
           data = vote)


pred.log1 <- predict(log1, vote, type = "response")
pred.log2 <- predict(log2, vote, type = "response")
pred.log3 <- predict(log3, vote, type = "response")
```

```{r}

set.seed(7)
train.Index <- caret::createDataPartition(vote$vote, p = 0.7, list = F)
vote.train <- vote[ train.Index,]
vote.test  <- vote[-train.Index,]

# features to be used for model training   
features <- c('vote', 'dem','rep','female','persfinance','natlecon', 
              'clintondis_fo', 'bushdis_fo','perotdis_fo','polID') 

# ----- Fitting a model ------ 
# Training classification decision tree
dt <- rpart(vote ~ ., 
            data = vote.train[,features], 
            method = "class",   #cause we have a classification problem
            parms = list(split = "information"),  # the splitting index 
            model = T) 

# ----- Deriving Predictions ------ 
# Predicting the instance of surviving
# first column  - probability of 0 for each observation
# second column - probability of 1
pred.dt <- predict(dt, newdata = vote.test, type = "prob")[, 2]
```


## Predictions

```{r}
Accuracy <- function(pred, real, threshold = 0.5){
  predClass <-  ifelse(pred > threshold, 1, 0)
  acc <- sum(predClass == real) / length(real)
  return(acc)
}
```

```{r}
(acc1 <- Accuracy(pred = pred.log1, real = vote$vote_num))
(acc2 <- Accuracy(pred = pred.log2, real = vote$vote_num))
(acc3 <- Accuracy(pred = pred.log3, real = vote$vote_num))
# Brier Score
(BS.log1 <- sqrt(mean((vote$vote_num - pred.log1)^2)))
(BS.log2 <- sqrt(mean((vote$vote_num- pred.log2)^2)))
(BS.log3 <- sqrt(mean((vote$vote_num - pred.log3)^2)))

```

```{r}
(accLasso <- Accuracy(pred = y.predlog_l1, real = y.test))
(accLRidge <- Accuracy(pred = y.predlog_r1, real = y.test))

(BS.logL1 <- sqrt(mean((y.test - y.predlog_l1)^2)))
(BS.logL2 <- sqrt(mean((y.test - y.predlog_r1)^2)))
```

```{r}
# ----- Evaluating Prediction Quality -----
# Calculate performance with AUC and RMSE
auc(vote.test$vote_num, pred.dt)
( rmse <- sqrt(mean((vote.test$vote_num - pred.dt)^2)) )
Accuracy(pred=pred.dt, real=vote.test$vote_num)

# Naive Classifier
baseline_probability <- sum(vote.train$vote_num == 1)/nrow(vote.train)
pred.baseline <- rep(baseline_probability, nrow(vote.test))

auc(vote.test$vote_num, pred.baseline)
( rmse <- sqrt(mean((vote.test$vote_num - pred.baseline)^2)) )
Accuracy(pred=pred.baseline, real=vote.test$vote_num)
```
```{r DT visual}
# Visualizing the results from "dt" using the prp() function
# default plot
prp(dt)
# prints the percentage of observations and class probabilities in each node
prp(dt, extra = 106, border.col = 0, box.palette="auto") 

```