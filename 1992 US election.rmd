---
title: "1992 U.S. Presidential election"
author: "Ali Tarek Maher Ibrahim Seada and Paul Lovis Maximilian Tr√ºstedt"
date: "24 6 2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Read the data into R environment
```{r}
library(pacman)
p_load(ggplot2,   # reportable graphs 
       cowplot,   # arranges ggplot graphs nicely
       stargazer, # nice tables
       glmnet,    # for regularization (lasso, ridge, elastic net)
       pROC)      # ROC AUC
rm(list=ls())
vote<-read.csv("vote92.csv")
str(vote)
summary(vote)
# ??remove cowplot, stargazer & pROC
```

## Preprocess the data, preparing it for the modeling
```{r}
vote$dem<-as.factor(vote$dem)
vote$rep<-as.factor(vote$rep)
vote$female<-as.factor(vote$female)
vote$persfinance<-as.factor(vote$persfinance)
vote$natlecon<-as.factor(vote$natlecon)
vote$polID<-as.factor((as.numeric(vote$dem)-1)+(as.numeric(vote$rep)*2-1))
str(vote)
```
We decided to change some of the numeric variables to factors, because it makes more sense to have them as categorical than as numeric variables.
Also this way, we can see, that there are no problems with the categorical variables regarding wrong values, because all provided levels are described by the given data set definition.
Additionally we create a categorical variable called polID to summarize which political party the respondent is identifying himself with.

- treat missing values
```{r}
colSums(is.na(vote))
```
There are no missing values in this data set. No NAs, as well as data, that could otherwise be identified as missing.

- handle sparse classes of categorical predictors
```{r}
table(vote$vote) # !!make these tables pretty (bar plot coloured)
table(vote$dem)
table(vote$rep)
table(vote$female)
table(vote$persfinance)
table(vote$natlecon) # ??leave fusion of 0 and 1, technically sparse
vote$natlecon[vote$natlecon==1]<-0
vote$natlecon[vote$natlecon==-1]<-1
vote$natlecon=droplevels(vote$natlecon)
table(vote$natlecon)
```
DEF X,O
X The categorical variables are simplified enough and don't need anything else to be done to them.
OR IF UPDATE LEFT
O We leave everything as is except for natlecon which has a sparse class regarding the level 1.
As solution we combine 0 and 1 as the level 0, meaning national economic conditions have gotten better or stayed the same over the last 12 months.
Level -1 gets changed to 1 as well which now means that conditions have gotten better.
The change from -1 to 1 is executed just because it is more common to have levels 0 and 1 instead of 0 and -1.

- take care of outliers, treat the skewed distributions and create new features 
```{r}
zScores<-function(var) {
    mu<-mean(var)
    sd<-sd(var)
    return((var-mu)/sd)
}

# treating clintondis
tp1<-ggplot(vote,aes(clintondis))+geom_boxplot()+coord_flip()
vote$clintondis_fo<-vote$clintondis
vote$clintondis_fo[zScores(vote$clintondis_fo)>1]<-
    round(mean(vote$clintondis_fo))+sd(vote$clintondis_fo)
tp2<-ggplot(vote,aes(clintondis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)

# treating bushdis
tp1<-ggplot(vote,aes(bushdis))+geom_boxplot()+coord_flip()
vote$bushdis_fo<-vote$bushdis
vote$bushdis_fo[zScores(vote$bushdis_fo)>2]<-
    round(mean(vote$bushdis_fo))+2*sd(vote$bushdis_fo)
tp2<-ggplot(vote,aes(bushdis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)

# treating perotdis
tp1<-ggplot(vote,aes(perotdis))+geom_boxplot()+coord_flip()
vote$perotdis_fo<-vote$perotdis
vote$perotdis_fo[zScores(vote$perotdis_fo)>1]<-
    round(mean(vote$perotdis_fo))+sd(vote$perotdis_fo)
tp2<-ggplot(vote,aes(perotdis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)

# !!make this shit beautiful
# !!reduce the outlier fixes with function
# ??passable results
# ??no skewed right
# ??do we actually need to fix outliers for the squared differences
```
There are a few outliers in the variables clintondis, bushdis and perotdis. We fix those outliers and save the fixed data in the variables called [original_var_name]_fo. The ending "fo" is derived from "fixed outliers".

- explore the relationships between predictors and the target
```{r}
ggplot(vote,aes(vote,fill=polID))+geom_bar() # !!fix colours and descriptions
# !!add percentiles to those splitted barplots somehow
```
___INSERT___

```{r}
ggplot(vote,aes(vote,fill=female))+geom_bar()
```
___INSERT___

```{r}
p1<-ggplot(vote,aes(vote,fill=persfinance))+geom_bar()
p2<-ggplot(vote,aes(vote,fill=natlecon))+geom_bar()
plot_grid(p1,p2,ncol=2)
# !!fix repeating barplot by creating a more diverse visual representation
```
___INSERT___

```{r}
p1<-ggplot(vote,aes(clintondis_fo,fill=vote))+geom_histogram(bins=3)
p2<-ggplot(vote,aes(bushdis_fo,fill=vote))+geom_histogram(bins=3)
p3<-ggplot(vote,aes(perotdis_fo,fill=vote))+geom_histogram(bins=3)
p4<-ggplot(vote,aes(clintondis,fill=vote))+geom_histogram(bins=3)
p5<-ggplot(vote,aes(bushdis,fill=vote))+geom_histogram(bins=3)
p6<-ggplot(vote,aes(perotdis,fill=vote))+geom_histogram(bins=3)
plot_grid(p1,p2,p3,p4,p5,p6,ncol=3)
# !!fix legend print and axis descriptions, also colours
```
___INSERT___

## Building the models
```{r}
# splitting the data into train and test
set.seed(777)
train.Index <-  sample(1:nrow(vote), round(0.7*nrow(vote)), replace = F)
# creating the train and test sets using train.Index 
vote.train <- vote[train.Index,]
vote.test  <- vote[-train.Index,]
  
# creating x and y for model training
# y - a target vector
y.train <- vote.train$vote_num
y.test  <- vote.test$vote_num

# X - a matrix with features/predictors 
features <- c('dem','rep','female','persfinance','natlecon', 
              'clintondis', 'bushdis','perotdis')


#model.matrix( ~ ., data = scoring.train[, features])
X.train <- model.matrix( ~ . -1, data = vote.train[, features])
X.test  <- model.matrix( ~ . -1, data = vote.test[, features])

```


1.  L1-nrom / Lasso

```{r}
log_l1 <- glmnet(X.train, y.train, alpha = 1)

plot(log_l1, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X.train), cex = .4)

plot(y = log_l1$dev.ratio, 
     x = log_l1$lambda,
     xlab = "lambda",
     ylab = "R-squared")


# selecting the optimal lambda
set.seed(77)
log_l1_cv <- cv.glmnet(X.train, y.train, alpha = 1, type.measure = "class", 
                       lambda = 10^seq(-5, 1, length.out = 100) , nfolds = 10)



y.predlog_l1 <-  predict(log_l1, newx = X.test, 
                         type = "response", s = log_l1_cv$lambda.min)
```

```{r}
# Setting alpha = 0 implements ridge regression
log_r1 <- glmnet(X.train, y.train, alpha = 0)


plot(log_r1, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X.test), cex = .3)


plot(y = log_r1$dev.ratio, 
     x = log_r1$lambda,
     xlab = "lambda",
     ylab = "R-squared")

# selecting the optimal lambda
set.seed(77)
log_r1_cv <- cv.glmnet(X.train, y.train, alpha = 0, type.measure = "class", 
                       lambda = 10^seq(-5, 1, length.out = 100), 
                        nfolds = 10)

y.predlog_r1 <-  predict(log_r1,    newx = X.test, 
                         type = "response", s = log_r1_cv$lambda.min)

```

```{r}
#only catagorical 

log1 <- glm(vote_num ~  dem + rep + female + persfinance + 
              natlecon + clintondis + persfinance + perotdis,
           data = vote)
#both
log2 <- glm(vote_num ~ dem + rep + female +persfinance +
          natlecon ,
           data = vote)

#only continous
log3 <- glm(vote_num ~ clintondis + bushdis +
              perotdis,
           data = vote)


pred.log1 <- predict(log1, vote, type = "response")
pred.log2 <- predict(log2, vote, type = "response")
pred.log3 <- predict(log3, vote, type = "response")
```

## Predictions

```{r}
Accuracy <- function(pred, real, threshold = 0.5){
  predClass <-  ifelse(pred > threshold, 1, 0)
  acc <- sum(predClass == real) / length(real)
  return(acc)
}
```

```{r}
(acc1 <- Accuracy(pred = pred.log1, real = vote$vote_num))
(acc2 <- Accuracy(pred = pred.log2, real = vote$vote_num))
(acc3 <- Accuracy(pred = pred.log3, real = vote$vote_num))
# Brier Score
(BS.log1 <- sqrt(mean((vote$vote_num - pred.log1)^2)))
(BS.log2 <- sqrt(mean((vote$vote_num- pred.log2)^2)))
(BS.log3 <- sqrt(mean((vote$vote_num - pred.log3)^2)))

```

```{r}
(accLasso <- Accuracy(pred = y.predlog_l1, real = y.test))
(accLRidge <- Accuracy(pred = y.predlog_r1, real = y.test))

(BS.logL1 <- sqrt(mean((y.test - y.predlog_l1)^2)))
(BS.logL2 <- sqrt(mean((y.test - y.predlog_r1)^2)))

```