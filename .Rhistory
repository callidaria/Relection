str(vote)
set.seed(7)
train.Index <- caret::createDataPartition(vote$vote_num, p = 0.7, list = F)
vote.train <- vote[ train.Index,]
vote.test  <- vote[-train.Index,]
# features to be used for model training
features <- c('vote_num', 'dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
# ----- Fitting a model ------
# Training classification decision tree
dt <- rpart(vote_num ~ . -Family,
data = vote.train[,features],
method = "class",   #cause we have a classification problem
parms = list(split = "information"),  # the splitting index
model = T)
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
p_load(ggplot2,   # reportable graphs
cowplot,   # arranges ggplot graphs nicely
stargazer, # nice tables
glmnet, # for regularization (lasso, ridge, elastic net)
caret,       #  splitting the data and more
rpart,       #  building decision trees
rpart.plot,
pROC)      # ROC AUC
rm(list=ls())
vote<-read.csv("vote92.csv", sep = ",", header = T,stringsAsFactors = T)
str(vote)
summary(vote)
# ??remove cowplot, stargazer & pROC
vote$vote_num <- as.numeric(vote$vote)
vote$dem<-as.factor(vote$dem)
vote$rep<-as.factor(vote$rep)
vote$female<-as.factor(vote$female)
vote$persfinance<-as.factor(vote$persfinance)
vote$natlecon<-as.factor(vote$natlecon)
vote$polID<-as.factor((as.numeric(vote$dem)-1)+(as.numeric(vote$rep)*2-1))
str(vote)
colSums(is.na(vote))
table(vote$vote) # !!make these tables pretty (bar plot coloured)
table(vote$dem)
table(vote$rep)
table(vote$female)
table(vote$persfinance)
table(vote$natlecon)
vote$natlecon[vote$natlecon==1]<-0
vote$natlecon[vote$natlecon==-1]<-1
vote$natlecon=droplevels(vote$natlecon)
table(vote$natlecon)
zScores<-function(var) {
mu<-mean(var)
sd<-sd(var)
return((var-mu)/sd)
}
# treating clintondis
tp1<-ggplot(vote,aes(clintondis))+geom_boxplot()+coord_flip()
vote$clintondis_fo<-vote$clintondis
vote$clintondis_fo[zScores(vote$clintondis_fo)>1]<-
round(mean(vote$clintondis_fo))+sd(vote$clintondis_fo)
tp2<-ggplot(vote,aes(clintondis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)
# treating bushdis
tp1<-ggplot(vote,aes(bushdis))+geom_boxplot()+coord_flip()
vote$bushdis_fo<-vote$bushdis
vote$bushdis_fo[zScores(vote$bushdis_fo)>2]<-
round(mean(vote$bushdis_fo))+2*sd(vote$bushdis_fo)
tp2<-ggplot(vote,aes(bushdis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)
# treating perotdis
tp1<-ggplot(vote,aes(perotdis))+geom_boxplot()+coord_flip()
vote$perotdis_fo<-vote$perotdis
vote$perotdis_fo[zScores(vote$perotdis_fo)>1]<-
round(mean(vote$perotdis_fo))+sd(vote$perotdis_fo)
tp2<-ggplot(vote,aes(perotdis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)
# !!make this shit beautiful
# !!reduce the outlier fixes with function
ggplot(vote,aes(vote,fill=polID))+geom_bar() # !!fix colours and descriptions
# !!add percentiles to those splitted barplots somehow
ggplot(vote,aes(vote,fill=female))+geom_bar()
p1<-ggplot(vote,aes(vote,fill=persfinance))+geom_bar()
p2<-ggplot(vote,aes(vote,fill=natlecon))+geom_bar()
plot_grid(p1,p2,ncol=2)
# !!fix repeating barplot by creating a more diverse visual representation
p1<-ggplot(vote,aes(clintondis_fo,fill=vote))+geom_histogram(bins=3)
p2<-ggplot(vote,aes(bushdis_fo,fill=vote))+geom_histogram(bins=3)
p3<-ggplot(vote,aes(perotdis_fo,fill=vote))+geom_histogram(bins=3)
p4<-ggplot(vote,aes(clintondis,fill=vote))+geom_histogram(bins=3)
p5<-ggplot(vote,aes(bushdis,fill=vote))+geom_histogram(bins=3)
p6<-ggplot(vote,aes(perotdis,fill=vote))+geom_histogram(bins=3)
plot_grid(p1,p2,p3,p4,p5,p6,ncol=3)
# !!fix legend print and axis descriptions, also colours
str(vote)
# splitting the data into train and test
set.seed(777)
train.Index <-  sample(1:nrow(vote), round(0.7*nrow(vote)), replace = F)
# creating the train and test sets using train.Index
vote.train <- vote[train.Index,]
vote.test  <- vote[-train.Index,]
# creating x and y for model training
# y - a target vector
y.train <- vote.train$vote_num
y.test  <- vote.test$vote_num
# X - a matrix with features/predictors
features <- c('dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
#model.matrix( ~ ., data = scoring.train[, features])
X.train <- model.matrix( ~ . -1, data = vote.train[, features])
X.test  <- model.matrix( ~ . -1, data = vote.test[, features])
log_l1 <- glmnet(X.train, y.train, alpha = 1)
plot(log_l1, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X.train), cex = .4)
plot(y = log_l1$dev.ratio,
x = log_l1$lambda,
xlab = "lambda",
ylab = "R-squared")
# selecting the optimal lambda
set.seed(77)
log_l1_cv <- cv.glmnet(X.train, y.train, alpha = 1, type.measure = "class",
lambda = 10^seq(-5, 1, length.out = 100) , nfolds = 10)
y.predlog_l1 <-  predict(log_l1, newx = X.test,
type = "response", s = log_l1_cv$lambda.min)
# Setting alpha = 0 implements ridge regression
log_r1 <- glmnet(X.train, y.train, alpha = 0)
plot(log_r1, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X.test), cex = .3)
plot(y = log_r1$dev.ratio,
x = log_r1$lambda,
xlab = "lambda",
ylab = "R-squared")
# selecting the optimal lambda
set.seed(77)
log_r1_cv <- cv.glmnet(X.train, y.train, alpha = 0, type.measure = "class",
lambda = 10^seq(-5, 1, length.out = 100),
nfolds = 10)
y.predlog_r1 <-  predict(log_r1,    newx = X.test,
type = "response", s = log_r1_cv$lambda.min)
#only catagorical
log1 <- glm(vote_num ~  dem + rep + female + persfinance +
natlecon + clintondis_fo + persfinance + perotdis_fo +bushdis_fo,
data = vote)
#both
log2 <- glm(vote_num ~ dem + rep + female +persfinance +
natlecon ,
data = vote)
#only continous
log3 <- glm(vote_num ~ clintondis_fo + bushdis_fo +
perotdis_fo,
data = vote)
pred.log1 <- predict(log1, vote, type = "response")
pred.log2 <- predict(log2, vote, type = "response")
pred.log3 <- predict(log3, vote, type = "response")
set.seed(7)
train.Index <- caret::createDataPartition(vote$vote_num, p = 0.7, list = F)
vote.train <- vote[ train.Index,]
vote.test  <- vote[-train.Index,]
# features to be used for model training
features <- c('vote_num', 'dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
# ----- Fitting a model ------
# Training classification decision tree
dt <- rpart(vote_num ~ . -clintondis -perotdis -perotdis,
data = vote.train[,features],
method = "class",   #cause we have a classification problem
parms = list(split = "information"),  # the splitting index
model = T)
set.seed(7)
train.Index <- caret::createDataPartition(vote$vote_num, p = 0.7, list = F)
vote.train <- vote[ train.Index,]
vote.test  <- vote[-train.Index,]
# features to be used for model training
features <- c('vote_num', 'dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
# ----- Fitting a model ------
# Training classification decision tree
dt <- rpart(vote_num ~ . -clintondis -bushdis -perotdis,
data = vote.train[,features],
method = "class",   #cause we have a classification problem
parms = list(split = "information"),  # the splitting index
model = T)
set.seed(7)
train.Index <- caret::createDataPartition(vote$vote_num, p = 0.7, list = F)
vote.train <- vote[ train.Index,]
vote.test  <- vote[-train.Index,]
# features to be used for model training
features <- c('vote_num', 'dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
# ----- Fitting a model ------
# Training classification decision tree
dt <- rpart(vote_num ~ .,
data = vote.train[,features],
method = "class",   #cause we have a classification problem
parms = list(split = "information"),  # the splitting index
model = T)
# ----- Deriving Predictions ------
# Predicting the instance of surviving
# first column  - probability of 0 for each observation
# second column - probability of 1
pred.dt <- predict(dt, newdata = vote.test, type = "prob")[, 2]
# ----- Evaluating Prediction Quality -----
# Calculate performance with AUC and RMSE
auc(vote.test$vote_num, pred.dt)
( rmse <- sqrt(mean((vote.test$vote_num - pred.dt)^2)) )
Accuracy(pred=pred.dt, real=vote.test$vote_num)
set.seed(7)
train.Index <- caret::createDataPartition(vote$vote, p = 0.7, list = F)
vote.train <- vote[ train.Index,]
vote.test  <- vote[-train.Index,]
# features to be used for model training
features <- c('vote', 'dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
# ----- Fitting a model ------
# Training classification decision tree
dt <- rpart(vote ~ .,
data = vote.train[,features],
method = "class",   #cause we have a classification problem
parms = list(split = "information"),  # the splitting index
model = T)
# ----- Deriving Predictions ------
# Predicting the instance of surviving
# first column  - probability of 0 for each observation
# second column - probability of 1
pred.dt <- predict(dt, newdata = vote.test, type = "prob")[, 2]
# ----- Evaluating Prediction Quality -----
# Calculate performance with AUC and RMSE
auc(vote.test$vote_num, pred.dt)
( rmse <- sqrt(mean((vote.test$vote_num - pred.dt)^2)) )
Accuracy(pred=pred.dt, real=vote.test$vote_num)
set.seed(7)
train.Index <- caret::createDataPartition(vote$vote, p = 0.7, list = F)
vote.train <- vote[ train.Index,]
vote.test  <- vote[-train.Index,]
# features to be used for model training
features <- c('vote', 'dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
# ----- Fitting a model ------
# Training classification decision tree
dt <- rpart(vote ~ .,
data = vote.train[,features],
method = "class",   #cause we have a classification problem
parms = list(split = "information"),  # the splitting index
model = T)
# ----- Deriving Predictions ------
# Predicting the instance of surviving
# first column  - probability of 0 for each observation
# second column - probability of 1
pred.dt <- predict(dt, newdata = vote.test, type = "prob")[, 2]
# ----- Evaluating Prediction Quality -----
# Calculate performance with AUC and RMSE
auc(vote.test$vote_num, pred.dt)
( rmse <- sqrt(mean((vote.test$vote_num - pred.dt)^2)) )
Accuracy(pred=pred.dt, real=vote.test$vote_num)
set.seed(7)
train.Index <- caret::createDataPartition(vote$vote, p = 0.7, list = F)
vote.train <- vote[ train.Index,]
vote.test  <- vote[-train.Index,]
# features to be used for model training
features <- c('vote', 'dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
# ----- Fitting a model ------
# Training classification decision tree
dt <- rpart(vote ~ .,
data = vote.train[,features],
method = "class",   #cause we have a classification problem
parms = list(split = "information"),  # the splitting index
model = T)
# ----- Deriving Predictions ------
# Predicting the instance of surviving
# first column  - probability of 0 for each observation
# second column - probability of 1
pred.dt <- predict(dt, newdata = vote.test, type = "prob")[, 2]
# ----- Evaluating Prediction Quality -----
# Calculate performance with AUC and RMSE
auc(vote.test$vote_num, pred.dt)
( rmse <- sqrt(mean((vote.test$vote_num - pred.dt)^2)) )
Accuracy(pred=pred.dt, real=vote.test$vote_num)
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
p_load(ggplot2,   # reportable graphs
cowplot,   # arranges ggplot graphs nicely
stargazer, # nice tables
glmnet, # for regularization (lasso, ridge, elastic net)
caret,       #  splitting the data and more
rpart,       #  building decision trees
rpart.plot,
pROC)      # ROC AUC
rm(list=ls())
vote<-read.csv("vote92.csv", sep = ",", header = T,stringsAsFactors = T)
str(vote)
summary(vote)
# ??remove cowplot, stargazer & pROC
vote$vote_num <- as.numeric(vote$vote)
vote$dem<-as.factor(vote$dem)
vote$rep<-as.factor(vote$rep)
vote$female<-as.factor(vote$female)
vote$persfinance<-as.factor(vote$persfinance)
vote$natlecon<-as.factor(vote$natlecon)
vote$polID<-as.factor((as.numeric(vote$dem)-1)+(as.numeric(vote$rep)*2-1))
str(vote)
colSums(is.na(vote))
table(vote$vote) # !!make these tables pretty (bar plot coloured)
table(vote$dem)
table(vote$rep)
table(vote$female)
table(vote$persfinance)
table(vote$natlecon)
vote$natlecon[vote$natlecon==1]<-0
vote$natlecon[vote$natlecon==-1]<-1
vote$natlecon=droplevels(vote$natlecon)
table(vote$natlecon)
zScores<-function(var) {
mu<-mean(var)
sd<-sd(var)
return((var-mu)/sd)
}
# treating clintondis
tp1<-ggplot(vote,aes(clintondis))+geom_boxplot()+coord_flip()
vote$clintondis_fo<-vote$clintondis
vote$clintondis_fo[zScores(vote$clintondis_fo)>1]<-
round(mean(vote$clintondis_fo))+sd(vote$clintondis_fo)
tp2<-ggplot(vote,aes(clintondis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)
# treating bushdis
tp1<-ggplot(vote,aes(bushdis))+geom_boxplot()+coord_flip()
vote$bushdis_fo<-vote$bushdis
vote$bushdis_fo[zScores(vote$bushdis_fo)>2]<-
round(mean(vote$bushdis_fo))+2*sd(vote$bushdis_fo)
tp2<-ggplot(vote,aes(bushdis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)
# treating perotdis
tp1<-ggplot(vote,aes(perotdis))+geom_boxplot()+coord_flip()
vote$perotdis_fo<-vote$perotdis
vote$perotdis_fo[zScores(vote$perotdis_fo)>1]<-
round(mean(vote$perotdis_fo))+sd(vote$perotdis_fo)
tp2<-ggplot(vote,aes(perotdis_fo))+geom_boxplot()+coord_flip()
plot_grid(tp1,tp2,ncol=2)
# !!make this shit beautiful
# !!reduce the outlier fixes with function
ggplot(vote,aes(vote,fill=polID))+geom_bar() # !!fix colours and descriptions
# !!add percentiles to those splitted barplots somehow
ggplot(vote,aes(vote,fill=female))+geom_bar()
p1<-ggplot(vote,aes(vote,fill=persfinance))+geom_bar()
p2<-ggplot(vote,aes(vote,fill=natlecon))+geom_bar()
plot_grid(p1,p2,ncol=2)
# !!fix repeating barplot by creating a more diverse visual representation
p1<-ggplot(vote,aes(clintondis_fo,fill=vote))+geom_histogram(bins=3)
p2<-ggplot(vote,aes(bushdis_fo,fill=vote))+geom_histogram(bins=3)
p3<-ggplot(vote,aes(perotdis_fo,fill=vote))+geom_histogram(bins=3)
p4<-ggplot(vote,aes(clintondis,fill=vote))+geom_histogram(bins=3)
p5<-ggplot(vote,aes(bushdis,fill=vote))+geom_histogram(bins=3)
p6<-ggplot(vote,aes(perotdis,fill=vote))+geom_histogram(bins=3)
plot_grid(p1,p2,p3,p4,p5,p6,ncol=3)
# !!fix legend print and axis descriptions, also colours
str(vote)
# splitting the data into train and test
set.seed(777)
train.Index <-  sample(1:nrow(vote), round(0.7*nrow(vote)), replace = F)
# creating the train and test sets using train.Index
vote.train <- vote[train.Index,]
vote.test  <- vote[-train.Index,]
# creating x and y for model training
# y - a target vector
y.train <- vote.train$vote_num
y.test  <- vote.test$vote_num
# X - a matrix with features/predictors
features <- c('dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
#model.matrix( ~ ., data = scoring.train[, features])
X.train <- model.matrix( ~ . -1, data = vote.train[, features])
X.test  <- model.matrix( ~ . -1, data = vote.test[, features])
log_l1 <- glmnet(X.train, y.train, alpha = 1)
plot(log_l1, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X.train), cex = .4)
plot(y = log_l1$dev.ratio,
x = log_l1$lambda,
xlab = "lambda",
ylab = "R-squared")
# selecting the optimal lambda
set.seed(77)
log_l1_cv <- cv.glmnet(X.train, y.train, alpha = 1, type.measure = "class",
lambda = 10^seq(-5, 1, length.out = 100) , nfolds = 10)
y.predlog_l1 <-  predict(log_l1, newx = X.test,
type = "response", s = log_l1_cv$lambda.min)
# Setting alpha = 0 implements ridge regression
log_r1 <- glmnet(X.train, y.train, alpha = 0)
plot(log_r1, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X.test), cex = .3)
plot(y = log_r1$dev.ratio,
x = log_r1$lambda,
xlab = "lambda",
ylab = "R-squared")
# selecting the optimal lambda
set.seed(77)
log_r1_cv <- cv.glmnet(X.train, y.train, alpha = 0, type.measure = "class",
lambda = 10^seq(-5, 1, length.out = 100),
nfolds = 10)
y.predlog_r1 <-  predict(log_r1,    newx = X.test,
type = "response", s = log_r1_cv$lambda.min)
#only catagorical
log1 <- glm(vote_num ~  dem + rep + female + persfinance +
natlecon + clintondis_fo + persfinance + perotdis_fo +bushdis_fo,
data = vote)
#both
log2 <- glm(vote_num ~ dem + rep + female +persfinance +
natlecon ,
data = vote)
#only continous
log3 <- glm(vote_num ~ clintondis_fo + bushdis_fo +
perotdis_fo,
data = vote)
pred.log1 <- predict(log1, vote, type = "response")
pred.log2 <- predict(log2, vote, type = "response")
pred.log3 <- predict(log3, vote, type = "response")
set.seed(7)
train.Index <- caret::createDataPartition(vote$vote, p = 0.7, list = F)
vote.train <- vote[ train.Index,]
vote.test  <- vote[-train.Index,]
# features to be used for model training
features <- c('vote', 'dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
# ----- Fitting a model ------
# Training classification decision tree
dt <- rpart(vote ~ .,
data = vote.train[,features],
method = "class",   #cause we have a classification problem
parms = list(split = "information"),  # the splitting index
model = T)
# ----- Deriving Predictions ------
# Predicting the instance of surviving
# first column  - probability of 0 for each observation
# second column - probability of 1
pred.dt <- predict(dt, newdata = vote.test, type = "prob")[, 2]
Accuracy <- function(pred, real, threshold = 0.5){
predClass <-  ifelse(pred > threshold, 1, 0)
acc <- sum(predClass == real) / length(real)
return(acc)
}
(acc1 <- Accuracy(pred = pred.log1, real = vote$vote_num))
(acc2 <- Accuracy(pred = pred.log2, real = vote$vote_num))
(acc3 <- Accuracy(pred = pred.log3, real = vote$vote_num))
# Brier Score
(BS.log1 <- sqrt(mean((vote$vote_num - pred.log1)^2)))
(BS.log2 <- sqrt(mean((vote$vote_num- pred.log2)^2)))
(BS.log3 <- sqrt(mean((vote$vote_num - pred.log3)^2)))
(accLasso <- Accuracy(pred = y.predlog_l1, real = y.test))
(accLRidge <- Accuracy(pred = y.predlog_r1, real = y.test))
(BS.logL1 <- sqrt(mean((y.test - y.predlog_l1)^2)))
(BS.logL2 <- sqrt(mean((y.test - y.predlog_r1)^2)))
# ----- Evaluating Prediction Quality -----
# Calculate performance with AUC and RMSE
auc(vote.test$vote_num, pred.dt)
( rmse <- sqrt(mean((vote.test$vote_num - pred.dt)^2)) )
Accuracy(pred=pred.dt, real=vote.test$vote_num)
# Naive Classifier
baseline_probability <- sum(vote.train$vote_num == 1)/nrow(vote.train)
pred.baseline <- rep(baseline_probability, nrow(vote.test))
auc(vote.test$vote_num, pred.baseline)
( rmse <- sqrt(mean((vote.test$vote_num - pred.baseline)^2)) )
Accuracy(pred=pred.baseline, real=vote.test$vote_num)
# Visualizing the results from "dt" using the prp() function
# default plot
prp(dt)
# prints the percentage of observations and class probabilities in each node
prp(dt, extra = 106, border.col = 0, box.palette="auto")
set.seed(7)
train.Index <- caret::createDataPartition(vote$vote, p = 0.7, list = F)
vote.train <- vote[ train.Index,]
vote.test  <- vote[-train.Index,]
# features to be used for model training
features <- c('vote', 'dem','rep','female','persfinance','natlecon',
'clintondis_fo', 'bushdis_fo','perotdis_fo','polID')
# ----- Fitting a model ------
# Training classification decision tree
dt <- rpart(vote ~ .,
data = vote.train[,features],
method = "class",   #cause we have a classification problem
parms = list(split = "information"),  # the splitting index
model = T)
# ----- Deriving Predictions ------
pred.dt <- predict(dt, newdata = vote.test, type = "prob")[, 2]
# Visualizing the results from "dt" using the prp() function
# default plot
prp(dt)
# prints the percentage of observations and class probabilities in each node
prp(dt, extra = 106, border.col = 0, box.palette="auto")
Accuracy <- function(pred, real, threshold = 0.5){
predClass <-  ifelse(pred > threshold, 1, 0)
acc <- sum(predClass == real) / length(real)
return(acc)
}
(acc1 <- Accuracy(pred = pred.log1, real = vote$vote_num))
(acc2 <- Accuracy(pred = pred.log2, real = vote$vote_num))
(acc3 <- Accuracy(pred = pred.log3, real = vote$vote_num))
# Brier Score
(BS.log1 <- sqrt(mean((vote$vote_num - pred.log1)^2)))
(BS.log2 <- sqrt(mean((vote$vote_num- pred.log2)^2)))
(BS.log3 <- sqrt(mean((vote$vote_num - pred.log3)^2)))
(accLasso <- Accuracy(pred = y.predlog_l1, real = y.test))
(accLRidge <- Accuracy(pred = y.predlog_r1, real = y.test))
(BS.logL1 <- sqrt(mean((y.test - y.predlog_l1)^2)))
(BS.logL2 <- sqrt(mean((y.test - y.predlog_r1)^2)))
# ----- Evaluating Prediction Quality -----
# Calculate performance with AUC and RMSE
auc(vote.test$vote_num, pred.dt)
( rmse <- sqrt(mean((vote.test$vote_num - pred.dt)^2)) )
Accuracy(pred=pred.dt, real=vote.test$vote_num)
# Naive Classifier
baseline_probability <- sum(vote.train$vote_num == 1)/nrow(vote.train)
pred.baseline <- rep(baseline_probability, nrow(vote.test))
auc(vote.test$vote_num, pred.baseline)
( rmse <- sqrt(mean((vote.test$vote_num - pred.baseline)^2)) )
Accuracy(pred=pred.baseline, real=vote.test$vote_num)
